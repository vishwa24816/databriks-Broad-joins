{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63b7c996-48c3-43c6-b2d0-0961ce504ebc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Large DataFrame (Fact Table)\n",
    "large_data = [\n",
    "    (1, \"Alice\", \"HR\"),\n",
    "    (2, \"Bob\", \"IT\"),\n",
    "    (3, \"Charlie\", \"FIN\"),\n",
    "    (4, \"David\", \"IT\"),\n",
    "    (5, \"Eve\", \"HR\"),\n",
    "    (6, \"Frank\", \"FIN\"),\n",
    "    (7, \"George\", \"HR\"),\n",
    "    (8, \"Henry\", \"FIN\"),\n",
    "    (9, \"Isla\", \"IT\"),\n",
    "    (10, \"Julia\", \"HR\")\n",
    "]\n",
    "large_df = spark.createDataFrame(large_data, [\"emp_id\", \"name\", \"dept_id\"])\n",
    "\n",
    "# Small DataFrame (Lookup Table)\n",
    "small_data = [\n",
    "    (\"HR\", \"Human Resources\"),\n",
    "    (\"IT\", \"Information Technology\"),\n",
    "    (\"FIN\", \"Finance & Accounts\")\n",
    "]\n",
    "small_df = spark.createDataFrame(small_data, [\"dept_id\", \"dept_name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13091a4c-379c-494a-9667-0534bff600e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored at: /Volumes/exp/join/j and /Volumes/exp/join/v\n"
     ]
    }
   ],
   "source": [
    "base = \"/Volumes/exp/join/\"\n",
    "\n",
    "large_path = base + \"j\"\n",
    "small_path = base + \"v\"\n",
    "\n",
    "large_df.write.mode(\"overwrite\").option(\"header\", True).csv(large_path)\n",
    "small_df.write.mode(\"overwrite\").option(\"header\", True).csv(small_path)\n",
    "\n",
    "print(\"Stored at:\", large_path, \"and\", small_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "118b2737-50ff-4797-a568-f34f4770c53b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------+\n|emp_id|   name|dept_id|\n+------+-------+-------+\n|     9|   Isla|     IT|\n|    10|  Julia|     HR|\n|     4|  David|     IT|\n|     5|    Eve|     HR|\n|     3|Charlie|    FIN|\n|     6|  Frank|    FIN|\n|     7| George|     HR|\n|     8|  Henry|    FIN|\n|     1|  Alice|     HR|\n|     2|    Bob|     IT|\n+------+-------+-------+\n\n+-------+--------------------+\n|dept_id|           dept_name|\n+-------+--------------------+\n|     IT|Information Techn...|\n|    FIN|  Finance & Accounts|\n|     HR|     Human Resources|\n+-------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "large_input = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(large_path)\n",
    "small_input = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(small_path)\n",
    "\n",
    "large_input.show()\n",
    "small_input.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af41bc73-b060-4efd-ab73-f014c6eda599",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+--------------------+\n|dept_id|emp_id|   name|           dept_name|\n+-------+------+-------+--------------------+\n|     IT|     9|   Isla|Information Techn...|\n|     HR|    10|  Julia|     Human Resources|\n|     IT|     4|  David|Information Techn...|\n|     HR|     5|    Eve|     Human Resources|\n|    FIN|     3|Charlie|  Finance & Accounts|\n|    FIN|     6|  Frank|  Finance & Accounts|\n|     HR|     7| George|     Human Resources|\n|    FIN|     8|  Henry|  Finance & Accounts|\n|     HR|     1|  Alice|     Human Resources|\n|     IT|     2|    Bob|Information Techn...|\n+-------+------+-------+--------------------+\n\n== Parsed Logical Plan ==\n'Join UsingJoin(Inner, [dept_id])\n:- Relation [emp_id#12235,name#12236,dept_id#12237] csv\n+- Relation [dept_id#12271,dept_name#12272] csv\n\n== Analyzed Logical Plan ==\ndept_id: string, emp_id: int, name: string, dept_name: string\nProject [dept_id#12237, emp_id#12235, name#12236, dept_name#12272]\n+- Join Inner, (dept_id#12237 = dept_id#12271)\n   :- Relation [emp_id#12235,name#12236,dept_id#12237] csv\n   +- Relation [dept_id#12271,dept_name#12272] csv\n\n== Optimized Logical Plan ==\nProject [dept_id#12237, emp_id#12235, name#12236, dept_name#12272]\n+- Join Inner, (dept_id#12237 = dept_id#12271)\n   :- Filter isnotnull(dept_id#12237)\n   :  +- Relation [emp_id#12235,name#12236,dept_id#12237] csv\n   +- Filter isnotnull(dept_id#12271)\n      +- Relation [dept_id#12271,dept_name#12272] csv\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Initial Plan ==\n   ColumnarToRow\n   +- PhotonResultStage\n      +- PhotonProject [dept_id#12237, emp_id#12235, name#12236, dept_name#12272]\n         +- PhotonBroadcastHashJoin [dept_id#12237], [dept_id#12271], Inner, BuildRight, false, true\n            :- PhotonFilter isnotnull(dept_id#12237)\n            :  +- PhotonRowToColumnar\n            :     +- FileScan csv [emp_id#12235,name#12236,dept_id#12237] Batched: false, DataFilters: [isnotnull(dept_id#12237)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/Volumes/exp/join/j], PartitionFilters: [], PushedFilters: [IsNotNull(dept_id)], ReadSchema: struct<emp_id:int,name:string,dept_id:string>\n            +- PhotonShuffleExchangeSource\n               +- PhotonShuffleMapStage EXECUTOR_BROADCAST, [id=#10607]\n                  +- PhotonShuffleExchangeSink SinglePartition\n                     +- PhotonFilter isnotnull(dept_id#12271)\n                        +- PhotonRowToColumnar\n                           +- FileScan csv [dept_id#12271,dept_name#12272] Batched: false, DataFilters: [isnotnull(dept_id#12271)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/Volumes/exp/join/v], PartitionFilters: [], PushedFilters: [IsNotNull(dept_id)], ReadSchema: struct<dept_id:string,dept_name:string>\n\n== Photon Explanation ==\nThe query is fully supported by Photon.\n"
     ]
    }
   ],
   "source": [
    "default_join = large_input.join(small_input, \"dept_id\")\n",
    "default_join.show()\n",
    "default_join.explain(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1d4677f-d02a-490a-a252-10fc75cf3685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+--------------------+\n|dept_id|emp_id|   name|           dept_name|\n+-------+------+-------+--------------------+\n|     IT|     9|   Isla|Information Techn...|\n|     HR|    10|  Julia|     Human Resources|\n|     IT|     4|  David|Information Techn...|\n|     HR|     5|    Eve|     Human Resources|\n|    FIN|     3|Charlie|  Finance & Accounts|\n|    FIN|     6|  Frank|  Finance & Accounts|\n|     HR|     7| George|     Human Resources|\n|    FIN|     8|  Henry|  Finance & Accounts|\n|     HR|     1|  Alice|     Human Resources|\n|     IT|     2|    Bob|Information Techn...|\n+-------+------+-------+--------------------+\n\n== Parsed Logical Plan ==\n'Join UsingJoin(Inner, [dept_id])\n:- Relation [emp_id#12235,name#12236,dept_id#12237] csv\n+- UnresolvedHint broadcast\n   +- Relation [dept_id#12271,dept_name#12272] csv\n\n== Analyzed Logical Plan ==\ndept_id: string, emp_id: int, name: string, dept_name: string\nProject [dept_id#12237, emp_id#12235, name#12236, dept_name#12272]\n+- Join Inner, (dept_id#12237 = dept_id#12271)\n   :- Relation [emp_id#12235,name#12236,dept_id#12237] csv\n   +- ResolvedHint (strategy=broadcast)\n      +- Relation [dept_id#12271,dept_name#12272] csv\n\n== Optimized Logical Plan ==\nProject [dept_id#12237, emp_id#12235, name#12236, dept_name#12272]\n+- Join Inner, (dept_id#12237 = dept_id#12271), rightHint=(strategy=broadcast), joinId=35\n   :- Filter isnotnull(dept_id#12237)\n   :  +- Relation [emp_id#12235,name#12236,dept_id#12237] csv\n   +- Filter isnotnull(dept_id#12271)\n      +- Relation [dept_id#12271,dept_name#12272] csv\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Initial Plan ==\n   ColumnarToRow\n   +- PhotonResultStage\n      +- PhotonProject [dept_id#12237, emp_id#12235, name#12236, dept_name#12272]\n         +- PhotonBroadcastHashJoin [dept_id#12237], [dept_id#12271], Inner, BuildRight, false, true\n            :- PhotonFilter isnotnull(dept_id#12237)\n            :  +- PhotonRowToColumnar\n            :     +- FileScan csv [emp_id#12235,name#12236,dept_id#12237] Batched: false, DataFilters: [isnotnull(dept_id#12237)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/Volumes/exp/join/j], PartitionFilters: [], PushedFilters: [IsNotNull(dept_id)], ReadSchema: struct<emp_id:int,name:string,dept_id:string>\n            +- PhotonShuffleExchangeSource\n               +- PhotonShuffleMapStage EXECUTOR_BROADCAST, [id=#10827]\n                  +- PhotonShuffleExchangeSink SinglePartition\n                     +- PhotonFilter isnotnull(dept_id#12271)\n                        +- PhotonRowToColumnar\n                           +- FileScan csv [dept_id#12271,dept_name#12272] Batched: false, DataFilters: [isnotnull(dept_id#12271)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/Volumes/exp/join/v], PartitionFilters: [], PushedFilters: [IsNotNull(dept_id)], ReadSchema: struct<dept_id:string,dept_name:string>\n\n== Photon Explanation ==\nThe query is fully supported by Photon.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "broadcast_join = large_input.join(broadcast(small_input), \"dept_id\")\n",
    "broadcast_join.show()\n",
    "broadcast_join.explain(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2090dfff-6ebc-4c7f-806c-eacae9597d52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Join Time: 0.78814 sec\nBroadcast Join Time: 0.456 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "default_join.collect()\n",
    "end = time.time()\n",
    "print(\"Default Join Time:\", round(end - start, 5), \"sec\")\n",
    "\n",
    "start = time.time()\n",
    "broadcast_join.collect()\n",
    "end = time.time()\n",
    "print(\"Broadcast Join Time:\", round(end - start, 5), \"sec\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Broadxast join",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}